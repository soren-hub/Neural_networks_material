{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf12d98",
   "metadata": {},
   "source": [
    "## Introducción a ML "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57857b0a",
   "metadata": {},
   "source": [
    "Primera implementación de ML fue para identificar correos spam (the spam filter)\n",
    "\n",
    "¿Cual seria el proceso para identificar spam sin ML? \n",
    "* Se identificarian palabras claves en el titulo del correo o en el nombre del emisor del correo (buscan patrones a mano)\n",
    "* Se escribe un algoritmo que en el caso de ifentificar alguno de estos patrones marque como spam el correo  \n",
    "* Se probaría el algoritmo y repetiría los pasos 1 y 2 hasta que fuera lo suficientemente bueno como para lanzarlo a producción.\n",
    "\n",
    "El problema se reduciria a escibir una serie de complejas reglas. \n",
    "\n",
    "En cambio con ML: \n",
    "Se deja al modelo reconocer los patrones que hay en un set previamente clasificado, generando el aprendizaje esperado.  \n",
    "\n",
    "<img src=\"img/diagrama1.png\" height=\"600px\" width=\"400px\" />\n",
    "\n",
    "ML tambien permite inmiscuirse en ambitos donde hacer algoritmos(normales) se vuelve muy complejo como es en reconocimiento del habla.\n",
    "Es mucho más simple y practico dejar que un ML reconozca los patrones de miles de grabaciones. \n",
    "\n",
    "La aplicación de técnicas de ML para profundizar en grandes cantidades de datos puede ayudar a descubrir patrones que que no eran evidentes de inmediato. Esto se denomina *data mining*.\n",
    "\n",
    "<img src=\"img/diagrama2.png\" height=\"600px\" width=\"400px\" />\n",
    "\n",
    "En resumen, ML es excelente para:\n",
    "\n",
    "* Problemas para los que las soluciones existentes requieren mucho ajuste o largas listas de reglas: un algoritmo de Machine Learning puede a menudo simplificar el código y rendir mejor que el enfoque tradicional. \n",
    "\n",
    "* Problemas complejos para los que el uso de problemas complejos para los que un enfoque tradicional no ofrece una buena solución: las mejores técnicas de aprendizaje automático pueden encontrar una solución.\n",
    "\n",
    "* Entornos fluctuantes: un sistema de Machine Learning puede adaptarse a los nuevos datos.\n",
    "\n",
    "* Obtener información sobre problemas complejos y grandes cantidades de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a19bd",
   "metadata": {},
   "source": [
    "## Ejemplos de aplicaciones \n",
    "\n",
    "* **Analizar imágenes de productos en una línea de producción para clasificarlos automáticamente**: \n",
    "\n",
    "Si se trata de la clasificación de imágenes, suele realizarse por medeo de redes neuronales convolucionales (CNN; capítulo 14).\n",
    "\n",
    "* **Detección de tumores en escáneres cerebrales**:\n",
    "\n",
    "Se trata de una segmentación semántica, en la que se clasifica cada píxel de la imagen (ya que queremos determinar la ubicación y la forma exactas de los tumores).\n",
    "\n",
    "* **Clasificación automática de noticias**: \n",
    "\n",
    "Se trata del procesamiento del lenguaje natural (PLN) y, más concretamente, de la clasificación de textos, que puede realizarse mediante redes neuronales recurrentes (RNN), CNN o Transformers (véase el capítulo 16).\n",
    "\n",
    "* **Marcar automáticamente los comentarios ofensivos en los foros de discusión**: \n",
    "\n",
    "También se trata de la clasificación de textos, utilizando las mismas herramientas de PNL.\n",
    "\n",
    "* **Resumir automáticamente documentos largos**: \n",
    "\n",
    "Se trata de una rama de la PNL denominada resumen de texto, que también utiliza las mismas herramientas. mismas herramientas.\n",
    "\n",
    "* **Crear un chatbot o un asistente personal**:\n",
    "Esto implica muchos componentes de PNL, incluyendo módulos de comprensión del lenguaje natural (NLU) y de respuesta a preguntas.\n",
    "\n",
    "* **Predicción de los ingresos de la empresa el año que viene, basándose en muchas métricas de rendimiento**: \n",
    "\n",
    "Si se trata de una tarea de regresión se puede abordar con cualquier modelo de regresión, como un modelo de Linear Regression o Polynomial Regression (véase el capítulo 4), una regresión SVM (véase el capítulo 5), un regresión Random Forest (véase el capítulo 7) o una red neuronal artificial (véase el capítulo 10). Si quieres tener en cuenta secuencias de métricas de rendimiento pasadas, puedes utilizar RNNs, CNNs o Transformers (consulta los capítulos 15 y 16).\n",
    "\n",
    "* **Hacer que tu aplicación reaccione a los comandos de voz**: \n",
    "\n",
    "Si se trata del reconocimiento de voz, se requiere el procesamiento de muestras de audio: al tratarse de secuencias largas y complejas, suelen procesarse con RNNs, CNNs o Transformers (véanse los capítulos 15 y 16). \n",
    "\n",
    "* **Detección de fraudes con tarjetas de crédito**:\n",
    "\n",
    "Se trata de la detección de anomalías (véase el capítulo 9). \n",
    "\n",
    "* **Segmentar a los clientes en función de sus compras para poder diseñar una estrategia de marketing diferente para cada segmento**:\n",
    "\n",
    "Se trata de la agrupación (véase el capítulo 9).\n",
    "\n",
    "* **Representar un conjunto de datos complejo y de gran dimensión en un diagrama claro y perspicaz**:\n",
    "\n",
    "Esto es la visualización de datos, que a menudo implica técnicas de reducción de la dimensionalidad (véase el capítulo 8).\n",
    "\n",
    "* **Recomendar un producto que puede interesar a un cliente, basándose en sus compras anteriores**:\n",
    " \n",
    "Se trata de un sistema de recomendación. Un enfoque consiste en alimentar una red neuronal artificial (véase el capítulo 10) con las compras anteriores (y otra información sobre el cliente) y hacer que emita la siguiente compra más probable. Esta red neuronal suele entrenarse con secuencias de compras anteriores de todos los clientes.\n",
    "\n",
    "* **Construir un bot inteligente para un juego**:\n",
    "\n",
    "Para ello, se suele utilizar el aprendizaje por refuerzo (RL; véase el Capítulo 18), que es una rama del aprendizaje automático que entrena a los agentes que entrena a los agentes (como los bots) para que elijan las acciones que maximizarán sus tiempo (por ejemplo, un bot puede obtener una recompensa cada vez que el jugador pierda algunos puntos de vida). El famoso programa AlphaGo que venció al campeón del mundo en el juego del Go fue construido utilizando RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c2922",
   "metadata": {},
   "source": [
    "## Tipos de sistemas de aprendizaje automático.\n",
    "Estos algoritmos se pueden clasificar bajo los siguientes criterios: \n",
    "\n",
    "* Si son supervisados o no (supervisados, no supervisados, semisupervisados y con refuerzo de refuerzo).\n",
    "\n",
    "* Si pueden aprender o no progresivamente sobre la marcha (aprendizaje online a aprendizaje por lotes)\n",
    "\n",
    "* Aprendizaje basado en instancias(clustering) frente a aprendizaje basado en modelos(regresiones)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0279cf",
   "metadata": {},
   "source": [
    "## Aprendizaje supervisado/no supervisado\n",
    "\n",
    "### Aprendizaje supervisado\n",
    "Una tarea típica de aprendizaje supervisado es la clasificación pero tambien puede cumplir con tareas de regresión, en el primer caso el  algoritmo aprende de un set de datos previamente clasificado(*labels*), extrayendo las caracteristicas principales que le permitiran realizar la tarea de clasificación con exito.\n",
    "\n",
    "<img src=\"img/diagrama3.png\" height=\"600px\" width=\"400px\" />\n",
    "\n",
    "En el otro caso predece un valor numérico objetivo, como el precio de un coche, dado un conjunto de features (kilometraje, edad, marca, etc.) denominadas *predictors*.\n",
    "\n",
    "**Nota**: Mucha gente utiliza las palabras *atributo* y *característica* indistintamente.\n",
    "\n",
    "Algunos *algoritmos de regresión* pueden utilizarse también para la *clasificación*, y viceversa.Por ejemplo, la regresión logística puede utilizar para clasificación, ya que puede dar un valor que corresponde a la probabilidad de pertenecer a una clase determinada o no (por ejemplo, 20% de posibilidades de ser spam).\n",
    "\n",
    "Algunos de los algoritmos de aprendizaje supervisado más importantes que se veran:\n",
    "\n",
    "* *k-Nearest Neighbors*\n",
    "* *Linear Regression*\n",
    "* *Logistic Regression*\n",
    "* *Support Vector Machines (SVMs)*\n",
    "* *Decision Trees and Random Forests*\n",
    "* *Neural networks*\n",
    "\n",
    "### Aprendizaje no supervisado. \n",
    "\n",
    "En el aprendizaje no supervisado, los datos de entrenamiento no están etiquetados. El sistema intenta aprender sin un maestro.\n",
    "\n",
    "Algunos de los algoritmos de aprendizaje no supervisado:\n",
    "\n",
    "* **Clustering**\n",
    "    * *K-Means*\n",
    "    * *DBSCAN*\n",
    "    * *Hierarchical Cluster Analysis (HCA)*\n",
    "* **Anomaly detection and novelty detection**\n",
    "    * *One-class SVM*\n",
    "    * *Isolation Forest*\n",
    "\n",
    "* **Visualization and dimensionality reduction**\n",
    "    * *Principal Component Analysis (PCA)*\n",
    "    * *Kernel PCA*\n",
    "    * *Locally Linear Embedding (LLE)*\n",
    "    * *t-Distributed Stochastic Neighbor Embedding (t-SNE)*\n",
    "* **Association rule learning**\n",
    "    * *Apriori*\n",
    "    * *Eclat*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034811e1",
   "metadata": {},
   "source": [
    "<p style='text-align: center;'> Ejemplos de tareas no supervisadas </p>\n",
    "\n",
    "* *Clustering.*\n",
    "\n",
    "Una imagen habla más que mil palabras.\n",
    "\n",
    "<img src=\"img/diagrama4.png\" height=\"600px\" width=\"400px\" />\n",
    "\n",
    "* *Agrupación jerárquica.* \n",
    "\n",
    "No la entendi XD\n",
    "\n",
    "* *Algoritmos de visualización* \n",
    "\n",
    "<img src=\"img/diagrama5.png\" height=\"1000px\" width=\"600px\" />\n",
    "\n",
    "\n",
    "\n",
    "* *Reducción de la dimensionalidad.*\n",
    "\n",
    "El kilometraje de un coche puede estar  fuertemente correlacionado con su edad, por lo que el algoritmo de reducción de la dimensionalidad de dimensionalidad los fusionará en una sola característica que represente el desgaste del coche. Esto se llama extracción de características.\n",
    "\n",
    "**Tip**|\n",
    ":----------------------------:|\n",
    "A menudo es una buena idea tratar de reducir la dimensión de sus datos de entrenamiento utilizando un algoritmo de reducción de la dimensionalidad antes de alimentar a otro algoritmo de aprendizaje automático (como un algoritmo de aprendizaje supervisado). **Se ejecutará mucho más rápido, los datos ocuparán menos espacio en el disco y en la memoria, y en algunos casos también puede tener un mejor rendimiento**.|\n",
    "\n",
    "\n",
    "* *Detección de anomalías.*\n",
    "\n",
    "Detectar transacciones inusuales con tarjetas de crédito para prevenir el fraude, detectar defectos de fabricación o eliminar automáticamente los valores atípicos de un conjunto de datos antes de alimentar otro algoritmo de aprendizaje. Durante el entrenamiento, al sistema se le muestran la mayoría de los casos normales, para que aprenda a reconocerlos; entonces, cuando ve un nuevo caso, puede decir si se parece a uno normal o si es probable que sea una anomalía. \n",
    "\n",
    "<img src=\"img/diagrama6.png\" height=\"1000px\" width=\"600px\" />\n",
    "\n",
    "* *Detección de la novedad.* \n",
    "\n",
    "Si tiene miles de fotos de perros y el 1% de estas fotos representan chihuahuas, un *algoritmo de detección de novedades* no debería tratar las nuevas fotos de chihuahuas como novedades. Por otro lado, los *algoritmos de detección de anomalías* pueden considerar que estos perros son tan raros y tan diferentes de otros perros que probablemente los clasificarían como anomalías.\n",
    "*Esta tarea requiere tener un conjunto de entrenamiento muy \"limpio\", sin ninguna instancia que se desee que el que el algoritmo detecte*.\n",
    "\n",
    "* *Aprendizaje de reglas de asociación.* \n",
    "\n",
    "Supongamos que tenemos un supermercado. La ejecución de una regla de asociación en sus registros de ventas puede revelar que las personas que compran salsa barbacoa y patatas fritas también tienden a comprar carne. Por lo tanto, es posible que quiera colocar estos artículos cerca unos de otros.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb8e5d3",
   "metadata": {},
   "source": [
    "## Aprendizaje semisupervisado.\n",
    "\n",
    "Debido a que la clasificación es una tarea puede llevar mucho tiempo y ser costosa. Se crearon los algoritmos semisupervisado que pueden trabajar con datos parcialmente etiquetados. \n",
    "\n",
    "<img src=\"img/diagrama7.png\" height=\"1000px\" width=\"600px\" />\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "Cuando subes todas tus fotos familiares al Google Fotos, éste reconoce automáticamente que la misma persona A aparece en las fotos 1, 5 y 11, mientras que otra persona B aparece en las fotos 2, 5 y 7. Esta es la parte parte no supervisada del algoritmo (clustering).Sólo hay que añadir una etiqueta por persona y es capaz de nombrar a todos en cada foto, lo que es útil para buscar fotos. \n",
    "\n",
    "Las *deep belief networks*(DBN) se basan en componentes no supervisados llamados *restricted Boltzmann machines* (RBM) apiladas unas sobre otras. Las RBM se entrenan secuencialmente de forma no supervisada, y luego todo el sistema se ajusta mediante técnicas de aprendizaje supervisado.\n",
    "(¿Qué es esto? No lo se... pero aparece en el libro. Habrá que investigar.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bad7e3",
   "metadata": {},
   "source": [
    "## Aprendizaje por refuerzo. \n",
    "\n",
    "El aprendizaje por refuerzo es una bestia muy diferente. El sistema de aprendizaje, llamado agente en este contexto, puede observar el entorno, seleccionar y realizar acciones, y obtener recompensas a cambio (o penalizaciones en forma de recompensas negativas, como se muestra en la Figura 1-12). A continuación, debe aprender por sí mismo cuál es la mejor estrategia, denominada *policy*, para obtener la mayor recompensa a lo largo del tiempo. Una política define qué acción debe elegir el agente cuando se encuentra en una situación determinada.\n",
    "\n",
    "<img src=\"img/diagrama8.png\" height=\"1000px\" width=\"600px\" />\n",
    "\n",
    "Ejemplo: AlphaGo :3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402427e4",
   "metadata": {},
   "source": [
    "## Aprendizaje por lotes y online.\n",
    "\n",
    "¿EL algoritmo de ML puede aprender de forma incremental a partir de un flujo de datos entrantes? \n",
    "\n",
    "### Aprendizaje por lotes.\n",
    "En el aprendizaje por lotes, el sistema es incapaz de aprender de forma incremental: debe ser entrenado utilizando todos los datos disponibles. Esto suele requerir mucho tiempo y recursos informáticos, por lo que suele hacerse fuera de línea. Primero se entrena el sistema, y luego se lanza a producción y se ejecuta sin aprender más; sólo aplica lo que ha aprendido. Esto se denomina aprendizaje offline.\n",
    "\n",
    "Si se tienen nuevos datos hay que entrenar el modelo todo de nuevo (:O) pero todo el proceso de creacion, evaluación y puesta en marcha de un algoritmo ML puede automatizarse con bastante facilidad, pero todo esto resulta muy costoso por lo que se debe hacer con un espacio de tiempo razonable.\n",
    "\n",
    "<img src=\"img/diagrama9.png\" height=\"1000px\" width=\"600px\" />\n",
    "\n",
    "Si tu sistema tiene que adaptarse a datos que cambian rápidamente (por ejemplo, para predecir los precios de las acciones), entonces necesitas una solución más reactiva. \n",
    "\n",
    "El aprendizaje por lotes requiere demasiados recursos si es que se tienen enormes cantidades de datos por lo que en ocaciones se hace imposible tener este tipo de algoritmos. \n",
    "\n",
    "Afortunadamente, una opción mejor en todos estos casos es utilizar algoritmos que sean capaces de aprender de forma incremental.\n",
    "\n",
    "### Aprendizaje online.\n",
    "\n",
    "El entrenamiento es de forma incremental alimentándolo con instancias de datos de forma secuencial (por ejemplo, las cotizaciones bursátiles), ya sea individualmente o en pequeños grupos denominados *mini-batches*. Cada paso de aprendizaje es rápido y barato. Es una buena opción si se tienen recursos informáticos limitados ya que una vez entrenado el modelo se pueden liberar espacio desechando los datos ocupados. \n",
    "\n",
    "También pueden utilizarse para entrenar sistemas en enormes conjuntos de datos que no caben en la memoria principal de una máquina (esto se llama aprendizaje fuera del núcleo).\n",
    "\n",
    "**ADVERTENCIA**|\n",
    ":----------------------------:|\n",
    "El aprendizaje fuera del núcleo se realiza normalmente fuera de línea (es decir, no en el sistema en vivo), por lo que el aprendizaje en línea puede ser un nombre confuso. Piense en ello como un aprendizaje incremental.\n",
    "\n",
    "Si se establece una tasa de aprendizaje alta, el sistema se adaptará rápidamente a los nuevos datos, pero también tenderá a olvidar rápidamente los datos antiguos (no se quiere que un filtro de spam que marque sólo los últimos tipos de spam que se le han mostrado). Por el contrario, si se establece una tasa de aprendizaje baja, el sistema tendrá más inercia, es decir, aprenderá más lentamente, pero también será menos sensible al ruido de los nuevos datos o a las secuencias de puntos de datos no representativos (valores atípicos).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ae347b",
   "metadata": {},
   "source": [
    "## Aprendizaje basado en instancias frente a aprendizaje basado en modelos. \n",
    "\n",
    "### Aprendizaje basado en instancias. \n",
    "\n",
    "Se establece una metrica(instancia) con la que es posible comparar los distintos ejemplos a travez de lo que se denomina *measure of similarity* y entorno a la proximidad que se tenga clasificar el nuevo elemento. Por ejemplo,en la Figura 1-15 la nueva instancia se  clasificaría como un triángulo porque la mayoría de los casos más similares pertenecen a esa clase.\n",
    "<img src=\"img/diagrama10.png\" height=\"1000px\" width=\"600px\" />\n",
    "\n",
    "### Aprendizaje basado en modelos.\n",
    "\n",
    "En este caso el algotirmo proyecta un modelo sobre los datos que permite hacer predicciones. \n",
    "<img src=\"img/diagrama11.png\" height=\"1000px\" width=\"600px\" />\n",
    "\n",
    "Por ejemplo: Tomemos los datos del *Better Life Index* del sitio web de la OCDE \n",
    "<img src=\"img/diagrama12.png\" height=\"1000px\" width=\"600px\" />\n",
    "En donde podemos apreciar que una tendencia a pesar de tener datos ruidosos.Parece que la satisfacción vital aumenta de forma más o menos lineal a medida que aumenta el PIB per cápita del país.\n",
    "Podemos plantear el siguiente modelo: \n",
    "$$\n",
    "\\text{life_satisfaction}  = θ_{0} + θ_{1} \\times \\text{GDP_per_capita}\n",
    "$$\n",
    "En donde $θ_{0}$ y $θ_{1}$ son dos parametros de nuestro modelo y al ajustarlos podemos encontrar cualquier de las siguientes rectas.\n",
    "<img src=\"img/diagrama13.png\" height=\"1000px\" width=\"600px\" />\n",
    "\n",
    "Ahora cual par de numeros es mejor para describir nuestros datos, para eso hacemos uso de un concepto llamdano *cost function*, la cual mide que tan alejados estan los puntos reales con los predichos, y la mision es minimizar esta distancia.\n",
    "4\n",
    "Entonces la magia del ML es que se define un modelo luego se le entregan los datos de entrenamiento y el algoritmo encuentra los parametros del modelo.\n",
    "<img src=\"img/diagrama14.png\" height=\"1000px\" width=\"600px\" />\n",
    "Si se quiere saber cual el el indice de satisfacción de un pais que no se encuentra en la lista solo se deberia ingregar el DGP per capita del pais al modelo y sabremos el indices.\n",
    "Ejemplo: Chipre tiene un GDP de 22.587 dólares entonces la predicción del modelo dice $4,85 + 22.587 × 4,91 × 10 = 5,96.$\n",
    "\n",
    "\n",
    "**Nota**|\n",
    ":----------------------------:|\n",
    "(ver libro pag.49)Tambien se pudo utilizar el modelo de aprendizaje basado en instacias.k-Nearest Neighbors regression (en este ejemplo, k = 3)\n",
    "\n",
    "Si el modelo no predice bien es posible que tenga que utilizar más atributos (tasa de empleo, salud, contaminación atmosférica, etc.), obtener más datos de entrenamiento o de mejor calidad, o quizás seleccionar un modelo más potente (por ejemplo, un modelo de regresión polinómica).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d17587",
   "metadata": {},
   "source": [
    "## Principales retos del aprendizaje automático\n",
    "\n",
    "¿Qué es lo que puede salir mal? Principalmente 2 cosas:\n",
    "* un mal algoritmo \n",
    "* malos datos\n",
    "\n",
    "Empecemos con ejemplos de datos malos.\n",
    "\n",
    "### Insuficiente cantidad de datos de formación. \n",
    "A diferencia de los humanos, para un modelo de ML se le hace muy dificil generalizar el aprendizaje por lo que necesita una gran cantidad de datos.  \n",
    "\n",
    "(Ver libro pag.51) Cuando se tienen muchos datos se facilita la tarea de elegir modelos ya que la mayoria de estos tendran resultados similares ( aunque siempre habra uno mejor que los otros), pero cuando se tienen pocos datos es muy importante escoger bien el modelo.  \n",
    "\n",
    "### Nonrepresentative Training Data. \n",
    "\n",
    "Si se desea tener un buen entrenamiento del modelos es indispensable qeu se tengan datos que sean representativos.Ver el siguiente ejemplo:\n",
    "<img src=\"img/diagrama15.png\" height=\"1000px\" width=\"600px\" />\n",
    "Los datos en rojo no se ocuparon para el entrenamiento del modelo, por lo que el entrenamiento de este no alcanzo para una buena generalización.\n",
    "Estos nuevos datos cambiarian las ideas que se tenia antes, al parecer no hay un aumento de satisfacción tan considerable(comparar Luxembourg con Brazil). Hay que tener extremo cuidado con los datos de los extremos, en este caso los paises muy ricos y muy pobres. A esto se llama *sampling bias* y puede afectar a grandes conjuntos de datos como a pequeños.\n",
    "\n",
    "(ver libro 54) Se deben procurar tener datos diversos.\n",
    "\n",
    "### Datos de baja calidad.\n",
    "Una de la labores más importantes del procesamiento de datos es limpiar los datos(y es la que consume más timepo). Casos en los que conviene limpiar los datos: \n",
    "* Valores atipicos, puede ser útil simplemente descartarlas o intentar corregir los errores manualmente.\n",
    "* Si a algunas instancias les faltan algunas características (por ejemplo, el 5% de sus clientes no especificaron su edad), debe decidir si quiere ignorar este atributo por completo, ignorar estas instancias, rellenar los valores que faltan (por ejemplo, con la mediana de edad) o entrenar un modelo con la característica y otro sin ella.\n",
    "\n",
    "### Características irrelevantes.\n",
    "Garbage in, garbage out. Es necesario tener *features* relevantes para entregar al modelo. Hay que procurar: \n",
    "* Selección de características (selección de las características más útiles para entrenar entre las existentes).\n",
    "\n",
    "* Extracción de características(combinar las características existentes para producir una más útil)\n",
    "\n",
    "* Creación de nuevas características mediante la recopilación de nuevos datos.\n",
    "\n",
    "### Overfitting de los datos de entrenamiento.\n",
    "Principalmente es la memorización de los datos, aprender incluso los patrones del ruido de estos, por lo que generalizar se torna una tarea imposible. \n",
    "\n",
    "**Advertencia**|\n",
    ":----------------------------:|\n",
    "El sobreajuste se produce cuando el modelo es demasiado complejo en relación con la cantidad y el ruido de los datos de entrenamiento. Estas son las posibles soluciones:\n",
    "* Simplificar el modelo seleccionando uno con menos parámetros \n",
    "* Reunir más datos de entrenamiento.\n",
    "* Reducir el ruido de los datos de entrenamiento (por ejemplo, corregir los errores de los datos y eliminar los valores atípicos).|\n",
    "\n",
    "La restricción de un modelo para simplificarlo y reducir el riesgo de sobreajuste se denomina regularización.\n",
    "No entendi la explicación que dio. \n",
    "<img src=\"img/diagrama16.png\" height=\"1000px\" width=\"600px\" />\n",
    "La cantidad de regularización que se aplica durante el aprendizaje puede controlarse mediante un hiperparámetro. Un hiperparámetro es un parámetro de un algoritmo de aprendizaje (no del modelo). \n",
    "El ajuste de los hiperparámetros es una parte importante de la construcción de un algoitmo de ML (verá un ejemplo detallado en el próximo capítulo).\n",
    "\n",
    "### Underfitting de los datos de entrenamiento.\n",
    "Se produce cuando su modelo es demasiado simple para aprender la estructura subyacente de los datos. Por ejemplo, un modelo lineal de satisfacción vital es propenso al infraajuste.\n",
    "Estas son las principales opciones para solucionar este problema:\n",
    "* Seleccionar un modelo más potente, con más parámetros. \n",
    "* Alimentar el algoritmo de aprendizaje con mejores características (ingeniería de características). ingeniería de características). \n",
    "* Reducir las restricciones del modelo (por ejemplo, reducir el hiperparámetro de regularización). hiperparámetro de regularización).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c05c00",
   "metadata": {},
   "source": [
    "## Testing and Validating\n",
    "\n",
    "Siempre se debe separar los datos en dos conjuntos, uno para entrenar y otro para pruebas, con este ultimo se puede medior una metrica llamada *generalization error* que nos dira que tan bueno es el modelo.\n",
    "\n",
    "**TIP**|\n",
    ":----------------------------:|\n",
    "Es comun divider los datos en 80% para entrenamiento y 20% para pruebas, pero todo va a depende de la cantidad de datos que se tengan, si se tiene 10 millones de datos probablemente bastara con un 1% de datos para pruebas.\n",
    "\n",
    "### Ajuste de hiperparámetros y selección de modelos\n",
    "\n",
    "Es posible verse tentado a encontrar los mejores hiperparamentros probando entrenar 100 modelos diferentes utilizando 100 valores distintos para este hiperparámetro y quedandose con el que de mejores resultados, pero hay que considerar que tener un porcentaje bajo en el error de generalización no implica necesariamente que sea bueno prediciendo nuevos datos, quizas los hiperparametros se ajustaron para ese conjunto particular de datos de entrenamiento. \n",
    "La posible solución a esto seria aplicar el metodo de validación por retención(separar la data de entrenamiento validación y pruebas) entonces primero se entrena(sin los datos de validación) para encontrar los hiperparametros que den el mejor resultado con los datos de validación. Una vez hecho esto se entrena el modelo de nuevo con los datos de validación y se obtiene el modelo final. Posibles problemas: \n",
    "* Conjunto de validación demasiado pequeño: las evaluaciones del modelo serán imprecisa, puede acabar seleccionando un modelo subóptimo por error.\n",
    "* Conjunto de validación demasiado grandes: el conjunto de entrenamiento restante será mucho más pequeño que el conjunto de entrenamiento completo. ¿Por qué es esto malo? Bueno, como el modelo final se entrenará con el conjunto de entrenamiento completo, no es ideal comparar modelos candidatos entrenados con un conjunto de entrenamiento mucho más pequeño. Sería como seleccionar al velocista más rápido para participar en una maratón.\n",
    "\n",
    "Nueva solución *cross-validation* repetida, aca se utiliza muchos conjuntos de validación pequeños.Cada modelo se evalúa una vez por conjunto de validación después de haber sido entrenado con el resto de los datos. Al promediar todas las evaluaciones de un modelo, se obtiene una medida mucho más precisa de su rendimiento. El problema esta vez seria el el tiempo de entrenamiento se multiplica por el número de conjuntos de validación.\n",
    "\n",
    "\n",
    "### Desajuste de datos.\n",
    "\n",
    "Es extremadamente importantes que los datos de validación y test sean representativos de los datos que se espera tener en producción: puedes barajarlas y poner la mitad en el conjunto de validación y la otra mitad en el conjunto de prueba (asegurándote de que no haya duplicados o casi duplicados en ambos conjuntos). Una buena tecnica es separar el conjunto de entrenamiento en entrenamiento y entrenamiento-desarrollo. Una vez entrenado el modelo (en el conjunto de entrenamiento, no en el conjunto de entrenamiento-desarrollo), puedes evaluarlo en el conjunto de entrenamiento-desarrollo. Si su rendimiento es bueno, el modelo no se está ajustando en exceso al conjunto de entrenamiento.Si el modelo tiene un mal rendimiento en el conjunto de entrenamiento-desarrollo, entonces debe haber ajustado en exceso el conjunto de entrenamiento, por lo que deberías intentar simplificar o regularizar el modelo, obtener más datos de entrenamiento y limpiar los datos de entrenamiento.\n",
    "Si su rendimiento es bajo en el conjunto de validación, el problema debe provenir del desajuste de los datos. Entonces en este caso se puede preprocesar los datos de entrenamientos para que se parezcan más a los datos que se ocupara en producción,y luego volver a entrenar el modelo.\n",
    "\n",
    "\n",
    "**NO FREE LUNCH THEOREM**|\n",
    ":----------------------------:|\n",
    "Un modelo es una versión simplificada de las observaciones. En este solo esta la contenida la infomacion relevante de los datos. Para decidir qué datos descartar y qué datos conservar, hay que hacer suposiciones. ENo hay ningún modelo que a priori garantice un mejor funcionamiento (de ahí el nombre del teorema).n un famoso artículo de 1996, David Wolpert demostró que si no se hace ninguna suposición sobre los datos, no hay ninguna razón para preferir un modelo a otro. Esto se llama el teorema de No Free Lunch (NFL). No hay ningún modelo que a priori garantice un mejor funcionamiento (de ahí el nombre del teorema). La única forma de saber con seguridad qué modelo es el mejor es evaluarlos todos. Como esto no es posible, en la práctica se hacen algunas suposiciones razonables sobre los datos y se evalúan sólo unos pocos modelos razonables. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_new",
   "language": "python",
   "name": "book_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
