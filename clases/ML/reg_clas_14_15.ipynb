{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "273c16b5",
   "metadata": {},
   "source": [
    "veo un inconveniente con la explicaci√≥n del resultado. Si miramos el resultado de la perdida para el modelo lineal, aparece en pantalla algo como 9.11362706964361e-08 (el resultado que me arroja cercano al del video) que es igual a 0.00000009113‚Ä¶ , es decir, que en realidad el modelo que genera menor perdida es el lineal. Por otro lado al comparar los coeficientes para la variable ‚Äúcorruption‚Äù en todos los modelos, se observa que el coeficiente para el modelo lineal es de 0.9996‚Ä¶ , osea casi 1!!. Luego esto indicar√≠a que la variable es estadisticamente muy significativa y por tanto el modelo lasso nos estar√≠a llevando hacia un resultado err√≥neo. Me gustar√≠a escuchar su opini√≥n.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c3ef68",
   "metadata": {},
   "source": [
    "Gracias por tu aporte, yo estoy de acuerdo en que el mejor modelo en realidad es el NO regularizado, el que hemos llamado Lineal (aunque los tres son lineales).\n",
    "\n",
    "Sobre lo que comentas del valor del coeficiente ‚Äòcorruption‚Äô cercano a 1, y que por eso sea ‚Äòestad√≠sticamente muy significativo‚Äô, no s√© si tenga mucho sentido decir eso. En este caso, los coeficientes (llamados aveces valores theta o beta) pueden tomar cualquier valor seg√∫n los datos y el modelo (en el modelo lineal equivalen al valor de la pendiente). Yy que yo recuerde lo que s√≠ se busca comprobar estad√≠sticamente es que los coeficientes sean distintos de 0 (la hip√≥tesis nula es que s√≠ lo son, y la alternativa es que NO), pero no necesariamente que sean igual a 1, s√≥lo que sean distintos de 0 (el qu√© tan distintos depende de los datos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d760e512",
   "metadata": {},
   "source": [
    "No es una conclusi√≥n tan sencilla de sacar con las pocas pruebas que se hicieron (1. por que son muy pocos datos y 2. por que no se modificaron los par√°metros en ning√∫n momento, solo fue un ejercicio ‚Äúilustrativo‚Äù para dar las nociones generales), ya que la importancia de los modelos con regularizaci√≥n se ve en su generalizaci√≥n cuando se tenga una mayor cantidad de datos a evaluar, adem√°s si mira las formulas como tal siempre que incluya regularizaci√≥n la comparaci√≥n con el modelo b√°sico no va a ser igual que buena recuerde que le esta induciendo un ‚Äúerror‚Äù de m√°s a la fuerza. Y por √∫ltimo lo que dice el compa√±ero es cierto que el valor sea 1 no tiene nada que ver con que sea ‚Äúestad√≠sticamente muy significativo‚Äù simplemente son la constante de la formula de regresi√≥n para saber si son estadisticamente significativo se tiene que realizar una prueba de hip√≥tesis como el menciona y a√∫n as√≠ no es una confirmaci√≥n absoluta es solo un indicativo con la informaci√≥n que se tiene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2870301",
   "metadata": {},
   "source": [
    "Evidentemente como dices hay un inconveniente con la explicaci√≥n. Abonando a lo que dicen los compa√±eros sobre la los features significativos y la prueba de hip√≥tesis ser√≠a una buena idea sacar los p-valores, para tener una mejor noci√≥n de lo que sucede. Ahora, para mi sorpresa estos no vienen en los modelos de sklearn, al parecer no va por ah√≠ su filosof√≠a.\n",
    "Entonces ser√≠a una buena idea que los obtengamos, si alguien me quiere ayudar a hacer una funci√≥n que calcule estos valores, d√≠game y trabajamos juntos.\n",
    "(Yo s√© que podemos usar R o alguna otra librer√≠a pero creo ser√≠a una buena pr√°ctica y sirve que estudiamos m√°s a fondo estos conceptos üòÑ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59985b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             rank       score        high         low         gdp      family  \\\n",
      "count  155.000000  155.000000  155.000000  155.000000  155.000000  155.000000   \n",
      "mean    78.000000    5.354019    5.452326    5.255713    0.984718    1.188898   \n",
      "std     44.888751    1.131230    1.118542    1.145030    0.420793    0.287263   \n",
      "min      1.000000    2.693000    2.864884    2.521116    0.000000    0.000000   \n",
      "25%     39.500000    4.505500    4.608172    4.374955    0.663371    1.042635   \n",
      "50%     78.000000    5.279000    5.370032    5.193152    1.064578    1.253918   \n",
      "75%    116.500000    6.101500    6.194600    6.006527    1.318027    1.414316   \n",
      "max    155.000000    7.537000    7.622030    7.479556    1.870766    1.610574   \n",
      "\n",
      "           lifexp     freedom  generosity  corruption    dystopia  \n",
      "count  155.000000  155.000000  155.000000  155.000000  155.000000  \n",
      "mean     0.551341    0.408786    0.246883    0.123120    1.850238  \n",
      "std      0.237073    0.149997    0.134780    0.101661    0.500028  \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.377914  \n",
      "25%      0.369866    0.303677    0.154106    0.057271    1.591291  \n",
      "50%      0.606042    0.437454    0.231538    0.089848    1.832910  \n",
      "75%      0.723008    0.516561    0.323762    0.153296    2.144654  \n",
      "max      0.949492    0.658249    0.838075    0.464308    3.117485  \n",
      "(155, 7)\n",
      "(155, 1)\n",
      "Linear Loss: 9.141036544005131e-08\n",
      "Lasso Loss:  0.043103283478320056\n",
      "Ridge Loss:  0.005406053346932207\n",
      "================================\n",
      "Coef LASSO\n",
      "[1.31369267 0.86536939 0.54683237 0.77438478 0.         0.22741407\n",
      " 0.89511704]\n",
      "================================\n",
      "Coef RIDGE\n",
      "[[1.08453787 0.95442369 0.87697787 0.89647759 0.61762039 0.76447975\n",
      "  0.95787118]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = pd.read_csv('felicidad.csv')\n",
    "    print(dataset.describe())\n",
    "\n",
    "    X = dataset[['gdp', 'family', 'lifexp', 'freedom' , 'corruption' , 'generosity', 'dystopia']]\n",
    "    y = dataset[['score']]\n",
    "\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25)\n",
    "\n",
    "    modelLinear = LinearRegression().fit(X_train, y_train)\n",
    "    y_predict_linear =  modelLinear.predict(X_test)\n",
    "\n",
    "    modelLasso = Lasso(alpha=0.02).fit(X_train, y_train)\n",
    "    y_predict_lasso = modelLasso.predict(X_test)\n",
    "\n",
    "    modelRidge = Ridge(alpha=1).fit(X_train, y_train)\n",
    "    y_predict_ridge = modelRidge.predict(X_test)\n",
    "\n",
    "    linear_loss = mean_squared_error(y_test, y_predict_linear)\n",
    "    print(\"Linear Loss:\", linear_loss)\n",
    "\n",
    "    lasso_loss = mean_squared_error(y_test, y_predict_lasso)\n",
    "    print(\"Lasso Loss: \", lasso_loss)\n",
    "\n",
    "    ridge_loss = mean_squared_error(y_test, y_predict_ridge)\n",
    "    print(\"Ridge Loss: \", ridge_loss)\n",
    "\n",
    "    print(\"=\"*32)\n",
    "    print(\"Coef LASSO\")\n",
    "    print(modelLasso.coef_)\n",
    "    \n",
    "    print(\"=\"*32)\n",
    "    print(\"Coef RIDGE\")\n",
    "    print(modelRidge.coef_)\n",
    "\n",
    "#implementacion_lasso_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a807de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book",
   "language": "python",
   "name": "book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
